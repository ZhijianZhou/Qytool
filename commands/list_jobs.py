"""åŠŸèƒ½: æŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨ï¼ˆåŒ…æ‹¬ Running / Pending / Failed ç­‰å…¨éƒ¨çŠ¶æ€ï¼‰"""
import json
from datetime import datetime, timezone
from typing import List, Dict, Tuple

from rich.table import Table
from rich.panel import Panel
from InquirerPy import inquirer

from raytool.utils.kube import run_kubectl, get_pods, group_pods_by_job, get_pod_role
from raytool.utils.ui import (
    console, colorize_status, print_warning, print_info, print_error,
)


def list_jobs(namespace: str):
    """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡ï¼ˆæŒ‰ PyTorchJob + Pod å…¨é‡å±•ç¤ºï¼‰"""
    print_info("æ­£åœ¨æŸ¥è¯¢æ‰€æœ‰ä»»åŠ¡...")
    console.print()

    # â”€â”€ 1. èŽ·å–æ‰€æœ‰ PyTorchJob â”€â”€
    pytorchjobs = _get_pytorchjobs(namespace)
    # â”€â”€ 2. èŽ·å–æ‰€æœ‰ Pod â”€â”€
    pods = get_pods(namespace)

    if not pytorchjobs and not pods:
        print_warning("å½“å‰æ²¡æœ‰ä»»ä½•ä»»åŠ¡æˆ– Pod")
        return

    # â”€â”€ 3. æŒ‰çŠ¶æ€åˆ†ç±» PyTorchJob â”€â”€
    running_jobs = []
    pending_jobs = []
    other_jobs = []

    for job in pytorchjobs:
        status = job["status"]
        if status in ("Running", "Succeeded"):
            running_jobs.append(job)
        elif status in ("Pending", "Creating", "Restarting"):
            pending_jobs.append(job)
        else:
            other_jobs.append(job)

    # â”€â”€ 4. çŠ¶æ€æ‘˜è¦ â”€â”€
    total = len(pytorchjobs)
    _print_status_summary(pytorchjobs)
    console.print()

    # â”€â”€ 5. å±•ç¤ºä»»åŠ¡è¡¨æ ¼ â”€â”€
    if pytorchjobs:
        _print_all_jobs_table(pytorchjobs, pods, namespace)
    else:
        # æ—  PyTorchJobï¼ŒæŒ‰ Pod åˆ†ç»„å±•ç¤º
        jobs = group_pods_by_job(pods)
        _print_pod_groups_table(jobs)

    console.print()

    # â”€â”€ 6. Pending ä»»åŠ¡ç‰¹åˆ«æç¤º â”€â”€
    if pending_jobs:
        _print_pending_alerts(pending_jobs, pods)

    # â”€â”€ 7. äº¤äº’æ“ä½œ â”€â”€
    if not pytorchjobs:
        return

    action = inquirer.select(
        message="éœ€è¦è¿›ä¸€æ­¥æ“ä½œå—ï¼Ÿ",
        choices=[
            {"name": "ðŸ” æŸ¥çœ‹æŸä¸ªä»»åŠ¡çš„ Pod è¯¦æƒ…", "value": "detail"},
            {"name": "ðŸ©º è¯Šæ–­ Pending ä»»åŠ¡", "value": "diagnose"},
            {"name": "âŒ è¿”å›ž", "value": "cancel"},
        ],
        pointer="â¯",
    ).execute()

    if action == "cancel":
        return
    elif action == "detail":
        _show_job_pod_detail(pytorchjobs, pods, namespace)
    elif action == "diagnose":
        _diagnose_pending(pending_jobs, pods, namespace)


def _get_pytorchjobs(namespace: str) -> List[Dict]:
    """èŽ·å–æ‰€æœ‰ PyTorchJobï¼Œè§£æžåŸºæœ¬ä¿¡æ¯"""
    rc, stdout, stderr = run_kubectl(
        ["get", "pytorchjobs", "-o", "json"],
        namespace,
        timeout=15,
    )
    if rc != 0:
        return []

    try:
        data = json.loads(stdout)
    except (json.JSONDecodeError, KeyError):
        return []

    jobs = []
    for item in data.get("items", []):
        metadata = item.get("metadata", {})
        spec = item.get("spec", {})
        status_block = item.get("status", {})

        name = metadata.get("name", "")
        creation = metadata.get("creationTimestamp", "")

        # å‰¯æœ¬æ•°
        replica_specs = spec.get("pytorchReplicaSpecs", {})
        master_replicas = replica_specs.get("Master", {}).get("replicas", 1)
        worker_replicas = replica_specs.get("Worker", {}).get("replicas", 0)
        total_nodes = master_replicas + worker_replicas

        # GPU æ•°ï¼ˆä»Žå®¹å™¨èµ„æºè¯·æ±‚ä¸­èŽ·å–ï¼‰
        try:
            gpu_per_node = int(
                replica_specs.get("Master", {}).get("template", {}).get("spec", {})
                .get("containers", [{}])[0]
                .get("resources", {}).get("requests", {}).get("nvidia.com/gpu", 0)
            )
        except (IndexError, ValueError):
            gpu_per_node = 0
        total_gpus = total_nodes * gpu_per_node

        # å®žä¾‹ç±»åž‹
        try:
            instance_type = (
                replica_specs.get("Master", {}).get("template", {}).get("spec", {})
                .get("nodeSelector", {}).get("node.kubernetes.io/instance-type", "-")
            )
        except (KeyError, AttributeError):
            instance_type = "-"

        # çŠ¶æ€æŽ¨æ–­
        conditions = status_block.get("conditions", [])
        job_status = _infer_job_status(conditions, status_block)

        # è¿è¡Œæ—¶é—´
        age = _calc_age_from_timestamp(creation)

        jobs.append({
            "name": name,
            "status": job_status,
            "creation": creation,
            "age": age,
            "master_replicas": master_replicas,
            "worker_replicas": worker_replicas,
            "total_nodes": total_nodes,
            "gpu_per_node": gpu_per_node,
            "total_gpus": total_gpus,
            "instance_type": instance_type,
        })

    return sorted(jobs, key=lambda j: j["creation"], reverse=True)


def _infer_job_status(conditions: list, status_block: dict) -> str:
    """ä»Ž PyTorchJob çš„ conditions æŽ¨æ–­å½“å‰çŠ¶æ€"""
    # ä¼˜å…ˆçœ‹ conditionsï¼ˆæœ€æ–°çš„ condition åœ¨æœ€åŽï¼‰
    for cond in reversed(conditions):
        cond_type = cond.get("type", "")
        cond_status = cond.get("status", "")
        reason = cond.get("reason", "")
        if cond_type == "Succeeded" and cond_status == "True":
            return "Succeeded"
        if cond_type == "Failed" and cond_status == "True":
            return "Failed"
        if cond_type == "Running" and cond_status == "True":
            return "Running"
        if cond_type == "Created" and cond_status == "True":
            return "Pending"
        if cond_type == "Restarting" and cond_status == "True":
            return "Restarting"

    # å›žé€€åˆ° replicaStatuses
    replica_statuses = status_block.get("replicaStatuses", {})
    has_active = False
    for role_status in replica_statuses.values():
        if role_status.get("active", 0) > 0:
            has_active = True
    if has_active:
        return "Running"

    return "Pending"


def _calc_age_from_timestamp(ts: str) -> str:
    """ä»Ž ISO æ—¶é—´æˆ³è®¡ç®— age å­—ç¬¦ä¸²"""
    if not ts:
        return "-"
    try:
        created = datetime.fromisoformat(ts.replace("Z", "+00:00"))
        now = datetime.now(timezone.utc)
        delta = now - created
        total_sec = int(delta.total_seconds())
        if total_sec < 60:
            return f"{total_sec}s"
        elif total_sec < 3600:
            return f"{total_sec // 60}m"
        elif total_sec < 86400:
            h = total_sec // 3600
            m = (total_sec % 3600) // 60
            return f"{h}h{m}m"
        else:
            d = total_sec // 86400
            h = (total_sec % 86400) // 3600
            return f"{d}d{h}h"
    except (ValueError, TypeError):
        return "-"


def _print_status_summary(pytorchjobs: list):
    """æ‰“å°çŠ¶æ€æ‘˜è¦é¢æ¿"""
    from collections import Counter
    status_counts = Counter(j["status"] for j in pytorchjobs)
    total = len(pytorchjobs)
    total_gpus = sum(j["total_gpus"] for j in pytorchjobs)

    parts = []
    for status in ["Running", "Pending", "Failed", "Succeeded", "Restarting", "Creating"]:
        cnt = status_counts.get(status, 0)
        if cnt > 0:
            parts.append(f"{colorize_status(status)}: {cnt}")

    summary_line = " | ".join(parts) if parts else "æ— ä»»åŠ¡"
    console.print(Panel(
        f"  ä»»åŠ¡æ€»æ•°: [bold]{total}[/bold]  |  GPU æ€»è®¡: [bold]{total_gpus}[/bold] å¼ \n"
        f"  {summary_line}",
        title="ðŸ“Š ä»»åŠ¡çŠ¶æ€æ‘˜è¦",
        border_style="cyan",
        padding=(0, 2),
    ))


def _print_all_jobs_table(pytorchjobs: list, pods: list, namespace: str):
    """æ‰“å°å…¨é‡ PyTorchJob ä»»åŠ¡è¡¨æ ¼"""
    # å°† Pod æŒ‰ä»»åŠ¡ååˆ†ç»„ï¼ˆç”¨äºŽç»Ÿè®¡å®žé™… Pod çŠ¶æ€ï¼‰
    pod_by_job = {}
    for p in pods:
        # PyTorchJob çš„ Pod åæ ¼å¼: {job-name}-{role}-{index}
        pname = p["name"]
        for job in pytorchjobs:
            jname = job["name"]
            if pname.startswith(jname + "-"):
                pod_by_job.setdefault(jname, []).append(p)
                break

    table = Table(title="ðŸ“‹ å…¨éƒ¨ä»»åŠ¡åˆ—è¡¨", show_lines=True, border_style="dim")
    table.add_column("#", style="dim", width=4)
    table.add_column("ä»»åŠ¡åç§°", style="bold cyan", min_width=30)
    table.add_column("çŠ¶æ€", justify="center", width=12)
    table.add_column("èŠ‚ç‚¹", justify="center", width=6)
    table.add_column("GPU", justify="center", width=8)
    table.add_column("å®žä¾‹ç±»åž‹", style="magenta", min_width=16)
    table.add_column("Pod çŠ¶æ€", min_width=24)
    table.add_column("è¿è¡Œæ—¶é—´", justify="center", width=10)

    for i, job in enumerate(pytorchjobs, 1):
        # ç»Ÿè®¡è¯¥ä»»åŠ¡ä¸‹ Pod çš„å„çŠ¶æ€æ•°
        job_pods = pod_by_job.get(job["name"], [])
        pod_status_str = _summarize_pod_statuses(job_pods, job["total_nodes"])

        instance_display = job["instance_type"].replace("ml.", "") if job["instance_type"] != "-" else "-"
        gpu_display = f"{job['total_gpus']}" if job["total_gpus"] > 0 else "-"

        table.add_row(
            str(i),
            job["name"],
            colorize_status(job["status"]),
            str(job["total_nodes"]),
            gpu_display,
            instance_display,
            pod_status_str,
            job["age"],
        )

    console.print(table)


def _summarize_pod_statuses(job_pods: list, expected_nodes: int) -> str:
    """å°† Pod çŠ¶æ€æ±‡æ€»ä¸ºç®€æ´å­—ç¬¦ä¸²ï¼Œå¦‚ '4 Running' æˆ– '2 Running, 2 Pending'"""
    if not job_pods:
        return f"[dim]0/{expected_nodes} Pod[/dim]"

    from collections import Counter
    counts = Counter(p["status"] for p in job_pods)
    parts = []
    for status in ["Running", "Pending", "ContainerCreating", "Failed", "Error", "CrashLoopBackOff", "Succeeded", "Unknown"]:
        cnt = counts.get(status, 0)
        if cnt > 0:
            parts.append(f"{cnt} {colorize_status(status)}")

    total_pods = len(job_pods)
    result = ", ".join(parts)
    if total_pods < expected_nodes:
        result += f" [dim]({total_pods}/{expected_nodes})[/dim]"
    return result


def _print_pod_groups_table(jobs: dict):
    """æ—  PyTorchJob æ—¶æŒ‰ Pod åˆ†ç»„å±•ç¤ºï¼ˆå…œåº•ï¼‰"""
    table = Table(title="ðŸ“‹ Pod åˆ†ç»„åˆ—è¡¨", show_lines=False, border_style="dim")
    table.add_column("#", style="dim", width=4)
    table.add_column("ä»»åŠ¡åç§°", style="bold cyan", min_width=25)
    table.add_column("çŠ¶æ€", justify="center", width=12)
    table.add_column("èŠ‚ç‚¹æ•°", justify="center", width=8)
    table.add_column("Master/Head", justify="center", width=12)
    table.add_column("Worker", justify="center", width=8)

    for i, (job_name, pods) in enumerate(sorted(jobs.items()), 1):
        statuses = set(p["status"] for p in pods)
        if "Running" in statuses:
            status = "Running"
        elif "Pending" in statuses:
            status = "Pending"
        elif "Failed" in statuses:
            status = "Failed"
        else:
            status = list(statuses)[0] if statuses else "Unknown"

        head_count = sum(1 for p in pods if get_pod_role(p) in ("Head", "Master"))
        worker_count = sum(1 for p in pods if get_pod_role(p) == "Worker")

        table.add_row(
            str(i),
            job_name,
            colorize_status(status),
            str(len(pods)),
            str(head_count),
            str(worker_count),
        )

    console.print(table)


def _print_pending_alerts(pending_jobs: list, pods: list):
    """Pending ä»»åŠ¡å‘Šè­¦"""
    console.print(Panel(
        f"[bold yellow]âš ï¸  æœ‰ {len(pending_jobs)} ä¸ªä»»åŠ¡å¤„äºŽ Pending çŠ¶æ€[/bold yellow]\n\n"
        + "\n".join(
            f"  [yellow]â€¢[/yellow] {j['name']}  ({j['total_nodes']} èŠ‚ç‚¹, {j['total_gpus']} GPU, ç­‰å¾… {j['age']})"
            for j in pending_jobs
        )
        + "\n\n[dim]  æç¤º: é€‰æ‹©ã€ŒðŸ©º è¯Šæ–­ Pending ä»»åŠ¡ã€æŸ¥çœ‹è¯¦ç»†åŽŸå› [/dim]",
        title="âš ï¸  Pending å‘Šè­¦",
        border_style="yellow",
        padding=(0, 2),
    ))
    console.print()


def _show_job_pod_detail(pytorchjobs: list, pods: list, namespace: str):
    """é€‰æ‹©æŸä¸ªä»»åŠ¡ï¼Œå±•ç¤ºå…¶ä¸‹æ‰€æœ‰ Pod çš„è¯¦ç»†çŠ¶æ€"""
    choices = []
    for job in pytorchjobs:
        label = f"{job['name']}  ({colorize_status(job['status'])}, {job['total_nodes']}èŠ‚ç‚¹, {job['age']})"
        choices.append({"name": label, "value": job["name"]})
    choices.append({"name": "âŒ è¿”å›ž", "value": "__cancel__"})

    selected = inquirer.select(
        message="é€‰æ‹©è¦æŸ¥çœ‹çš„ä»»åŠ¡",
        choices=choices,
        pointer="â¯",
    ).execute()

    if selected == "__cancel__":
        return

    # è¿‡æ»¤è¯¥ä»»åŠ¡çš„ Pod
    job_pods = [p for p in pods if p["name"].startswith(selected + "-")]

    if not job_pods:
        print_warning(f"ä»»åŠ¡ {selected} ä¸‹æ²¡æœ‰æ‰¾åˆ° Pod")
        return

    console.print()
    table = Table(title=f"ðŸ“‹ {selected} â€” Pod è¯¦æƒ…", show_lines=True, border_style="cyan")
    table.add_column("#", style="dim", width=4)
    table.add_column("Pod åç§°", style="cyan", min_width=40)
    table.add_column("è§’è‰²", justify="center", width=8)
    table.add_column("çŠ¶æ€", justify="center", width=18)
    table.add_column("READY", justify="center", width=8)
    table.add_column("é‡å¯", justify="center", width=6)
    table.add_column("åˆ›å»ºæ—¶é—´", width=22)

    for i, pod in enumerate(sorted(job_pods, key=lambda p: p["name"]), 1):
        role = pod.get("role", "Unknown")
        # Master/Worker æŽ¨æ–­
        if role == "Unknown":
            if "-master-" in pod["name"]:
                role = "Master"
            elif "-worker-" in pod["name"]:
                role = "Worker"

        table.add_row(
            str(i),
            pod["name"],
            role,
            colorize_status(pod["status"]),
            pod["ready"],
            str(pod["restarts"]),
            pod["creation"][:19].replace("T", " ") if pod["creation"] else "-",
        )

    console.print(table)


def _diagnose_pending(pending_jobs: list, pods: list, namespace: str):
    """è¯Šæ–­ Pending ä»»åŠ¡ï¼šèŽ·å–äº‹ä»¶ä¿¡æ¯å±•ç¤ºåŽŸå› """
    if not pending_jobs:
        print_warning("å½“å‰æ²¡æœ‰ Pending çš„ä»»åŠ¡")
        return

    # é€‰æ‹©è¦è¯Šæ–­çš„ä»»åŠ¡
    if len(pending_jobs) == 1:
        target = pending_jobs[0]
    else:
        choices = []
        for job in pending_jobs:
            choices.append({
                "name": f"{job['name']}  ({job['total_nodes']}èŠ‚ç‚¹, ç­‰å¾… {job['age']})",
                "value": job["name"],
            })
        choices.append({"name": "ðŸ“‹ è¯Šæ–­å…¨éƒ¨", "value": "__all__"})
        choices.append({"name": "âŒ è¿”å›ž", "value": "__cancel__"})

        selected = inquirer.select(
            message="é€‰æ‹©è¦è¯Šæ–­çš„ä»»åŠ¡",
            choices=choices,
            pointer="â¯",
        ).execute()

        if selected == "__cancel__":
            return

        if selected == "__all__":
            for job in pending_jobs:
                _diagnose_single_job(job, pods, namespace)
            return

        target = next((j for j in pending_jobs if j["name"] == selected), None)
        if not target:
            return

    _diagnose_single_job(target, pods, namespace)


def _diagnose_single_job(job: dict, pods: list, namespace: str):
    """è¯Šæ–­å•ä¸ª Pending ä»»åŠ¡"""
    console.print()
    console.print(f"[bold cyan]ðŸ©º è¯Šæ–­: {job['name']}[/bold cyan]")
    console.print(f"  çŠ¶æ€: {colorize_status(job['status'])}  |  èŠ‚ç‚¹: {job['total_nodes']}  |  GPU: {job['total_gpus']}  |  ç­‰å¾…: {job['age']}")
    console.print()

    # æ‰¾åˆ°è¯¥ä»»åŠ¡çš„ä¸€ä¸ª Pending Podï¼ŒæŸ¥çœ‹äº‹ä»¶
    job_pods = [p for p in pods if p["name"].startswith(job["name"] + "-") and p["status"] == "Pending"]

    if not job_pods:
        # æ²¡æœ‰ Pending Podï¼Œå¯èƒ½ Pod è¿˜æ²¡åˆ›å»ºå‡ºæ¥
        print_warning("æœªæ‰¾åˆ° Pending Podï¼Œä»»åŠ¡å¯èƒ½è¿˜æœªåˆ›å»º Pod")
        # æŸ¥çœ‹ PyTorchJob äº‹ä»¶
        print_info(f"æ­£åœ¨èŽ·å– PyTorchJob äº‹ä»¶...")
        rc, stdout, stderr = run_kubectl(
            ["describe", "pytorchjob", job["name"]],
            namespace,
            timeout=15,
        )
        if rc == 0:
            # æå– Events éƒ¨åˆ†
            _print_events_section(stdout, job["name"])
        else:
            print_error(f"èŽ·å–äº‹ä»¶å¤±è´¥: {stderr.strip()}")
        return

    # å–ç¬¬ä¸€ä¸ª Pending Pod æŸ¥çœ‹äº‹ä»¶
    target_pod = job_pods[0]["name"]
    print_info(f"æ­£åœ¨èŽ·å– Pod äº‹ä»¶: {target_pod}")

    rc, stdout, stderr = run_kubectl(
        ["describe", "pod", target_pod],
        namespace,
        timeout=15,
    )
    if rc != 0:
        print_error(f"èŽ·å– Pod ä¿¡æ¯å¤±è´¥: {stderr.strip()}")
        return

    _print_events_section(stdout, target_pod)


def _print_events_section(describe_output: str, resource_name: str):
    """ä»Ž kubectl describe è¾“å‡ºä¸­æå–å¹¶å±•ç¤º Events éƒ¨åˆ†"""
    lines = describe_output.split("\n")
    events_start = -1
    for i, line in enumerate(lines):
        if line.strip().startswith("Events:"):
            events_start = i
            break

    if events_start == -1:
        print_warning("æœªæ‰¾åˆ°äº‹ä»¶ä¿¡æ¯")
        return

    events_lines = lines[events_start:]

    # è§£æžäº‹ä»¶ï¼Œé«˜äº® Warning
    formatted = []
    for line in events_lines:
        stripped = line.strip()
        if stripped.startswith("Warning") or "Warning" in stripped:
            formatted.append(f"  [bold yellow]{stripped}[/bold yellow]")
        elif stripped.startswith("Normal"):
            formatted.append(f"  [dim]{stripped}[/dim]")
        elif stripped.startswith("Events:"):
            formatted.append(f"  [bold]{stripped}[/bold]")
        elif stripped.startswith("Type") and "Reason" in stripped:
            formatted.append(f"  [bold]{stripped}[/bold]")
        elif stripped.startswith("----"):
            formatted.append(f"  [dim]{stripped}[/dim]")
        else:
            formatted.append(f"  {stripped}")

    console.print(Panel(
        "\n".join(formatted),
        title=f"ðŸ“‹ {resource_name} äº‹ä»¶",
        border_style="yellow",
        padding=(0, 1),
    ))
    console.print()
