"""åŠŸèƒ½: GPU å å¡ â€” æŸ¥è¯¢ç©ºé—²èŠ‚ç‚¹å¹¶è‡ªåŠ¨æäº¤/åˆ é™¤å å¡ä»»åŠ¡"""
import os
import re
import json
import glob
import yaml
import random
import time
import signal
import tempfile
from datetime import datetime
from typing import List, Dict, Tuple

from InquirerPy import inquirer
from rich.table import Table
from rich.panel import Panel

from raytool.utils.kube import run_kubectl, apply_yaml, get_pods, group_pods_by_job
from raytool.utils.ui import (
    console, confirm, confirm_with_input, print_success, print_error, print_warning, print_info,
    colorize_status,
)

# å å¡ä»»åŠ¡åå‰ç¼€ï¼ˆæ–°æ ¼å¼: run-{model}-{task}-{date}-{index}ï¼‰
OCCUPY_PREFIX = "run-"
# åŒ¹é…å å¡ä»»åŠ¡åçš„æ­£åˆ™: run-{model}-{task}-{MMDD}-{NN}
OCCUPY_NAME_PATTERN = re.compile(r"^run-[a-z0-9]+-[a-z0-9]+-\d{4}-\d{2}$")
# å¯é€‰çš„ä»»åŠ¡ç±»å‹
TASK_TYPES = ["retool", "search", "swebench"]
# å¯é€‰çš„æ¨¡å‹å
MODEL_NAMES = ["qwen3", "qwen25"]
# é»˜è®¤æ¯æ‰¹å ç”¨çš„èŠ‚ç‚¹æ•°ï¼ˆ1 Master + 3 Worker = 4 èŠ‚ç‚¹ï¼‰
DEFAULT_BATCH_SIZE = 4

# â”€â”€ ä¸åŒ GPU å®ä¾‹ç±»å‹çš„èµ„æºé…ç½® â”€â”€
# æ¯ä¸ªå®ä¾‹ç±»å‹å¯¹åº”: gpus_per_node, efa_count, ç®€çŸ­æè¿°
GPU_INSTANCE_PROFILES: Dict[str, Dict] = {
    "ml.p5en.48xlarge": {
        "gpus": 8,
        "efa": 16,
        "gpu_model": "H200",
        "description": "8Ã—H200 (p5en)",
    },
    "ml.p5e.48xlarge": {
        "gpus": 8,
        "efa": 32,
        "gpu_model": "H200",
        "description": "8Ã—H200 (p5e)",
    },
    "ml.p5.48xlarge": {
        "gpus": 8,
        "efa": 32,
        "gpu_model": "H100",
        "description": "8Ã—H100 (p5)",
    },
    "ml.p4d.24xlarge": {
        "gpus": 8,
        "efa": 4,
        "gpu_model": "A100",
        "description": "8Ã—A100 (p4d)",
    },
    "ml.p4de.24xlarge": {
        "gpus": 8,
        "efa": 4,
        "gpu_model": "A100-80G",
        "description": "8Ã—A100-80G (p4de)",
    },
    "ml.g5.48xlarge": {
        "gpus": 8,
        "efa": 1,
        "gpu_model": "A10G",
        "description": "8Ã—A10G (g5)",
    },
}

# æœªçŸ¥å®ä¾‹ç±»å‹çš„é»˜è®¤é…ç½®ï¼ˆå…œåº•ï¼‰
DEFAULT_GPU_PROFILE = {
    "gpus": 8,
    "efa": 16,
    "gpu_model": "Unknown",
    "description": "Unknown GPU",
}


def _get_instance_profile(instance_type: str) -> Dict:
    """è·å–å®ä¾‹ç±»å‹å¯¹åº”çš„èµ„æºé…ç½®ï¼ŒæœªçŸ¥ç±»å‹ä½¿ç”¨é»˜è®¤å€¼"""
    return GPU_INSTANCE_PROFILES.get(instance_type, DEFAULT_GPU_PROFILE)
# è‡ªåŠ¨å·¡é€»é»˜è®¤é—´éš”ï¼ˆç§’ï¼‰
DEFAULT_PATROL_INTERVAL = 300  # 5 åˆ†é’Ÿ
# å å¡ YAML å­˜æ”¾ç›®å½•
OCCUPY_YAML_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "occupy-jobs")
# å å¡ YAML æ¨¡æ¿è·¯å¾„
OCCUPY_TEMPLATE = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "ray-job", "zzj-gpu-occupy.yaml")


def occupy_gpus(namespace: str):
    """GPU å å¡äº¤äº’å¼å…¥å£ â€” å…ˆé€‰æ‹©æ“ä½œç±»å‹"""
    action = inquirer.select(
        message="ä¸»äººï¼Œè¯·é€‰æ‹©å å¡æ“ä½œ",
        choices=[
            {"name": "ğŸš€ æäº¤æ–°çš„å å¡ä»»åŠ¡", "value": "submit"},
            {"name": "ğŸ—‘ï¸  åˆ é™¤å·²æœ‰å å¡ä»»åŠ¡", "value": "delete"},
            {"name": "ğŸ‘ï¸  è‡ªåŠ¨å·¡é€» (å®šæ—¶æ£€æµ‹ç©ºé—²å¡å¹¶è‡ªåŠ¨å å¡)", "value": "patrol"},
            {"name": "âŒ è¿”å›", "value": "cancel"},
        ],
        pointer="â¯",
    ).execute()

    if action == "cancel":
        return
    elif action == "delete":
        _delete_occupy_jobs(namespace)
        return
    elif action == "patrol":
        _auto_patrol(namespace)
        return

    # â”€â”€ æäº¤æ–°å å¡ä»»åŠ¡ â”€â”€
    _submit_occupy_jobs(namespace)


def _delete_occupy_jobs(namespace: str):
    """æ‰¹é‡åˆ é™¤å å¡ä»»åŠ¡ï¼ˆPyTorchJobï¼‰"""
    print_info("æ­£åœ¨æŸ¥è¯¢å·²æœ‰çš„å å¡ä»»åŠ¡...")
    console.print()

    # æŸ¥è¯¢æ‰€æœ‰ PyTorchJob
    rc, stdout, stderr = run_kubectl(
        ["get", "pytorchjobs", "-o", "json"],
        namespace,
        timeout=15,
    )

    if rc != 0:
        print_error(f"æŸ¥è¯¢ PyTorchJob å¤±è´¥: {stderr.strip()}")
        return

    try:
        data = json.loads(stdout)
        items = data.get("items", [])
    except (json.JSONDecodeError, KeyError):
        print_error("è§£æ PyTorchJob åˆ—è¡¨å¤±è´¥")
        return

    if not items:
        print_warning("å½“å‰æ²¡æœ‰ä»»ä½• PyTorchJob")
        return

    # æŒ‰åç§°åŒ¹é…å å¡å‰ç¼€ï¼ŒåŒæ—¶ä¹Ÿåˆ—å‡ºæ‰€æœ‰ä»»åŠ¡è®©ç”¨æˆ·é€‰
    occupy_jobs = []
    other_jobs = []
    for item in items:
        name = item.get("metadata", {}).get("name", "")
        creation = item.get("metadata", {}).get("creationTimestamp", "")
        # ç»Ÿè®¡ Pod æ•°å’Œ GPU æ•°
        master_spec = item.get("spec", {}).get("pytorchReplicaSpecs", {}).get("Master", {})
        worker_spec = item.get("spec", {}).get("pytorchReplicaSpecs", {}).get("Worker", {})
        master_replicas = master_spec.get("replicas", 1)
        worker_replicas = worker_spec.get("replicas", 0)
        total_nodes = master_replicas + worker_replicas
        # ä»å®¹å™¨èµ„æºè¯·æ±‚ä¸­è·å–æ¯èŠ‚ç‚¹ GPU æ•°
        try:
            gpu_per_node = int(
                master_spec.get("template", {}).get("spec", {}).get("containers", [{}])[0]
                .get("resources", {}).get("requests", {}).get("nvidia.com/gpu", 8)
            )
        except (IndexError, ValueError):
            gpu_per_node = 8
        total_gpus = total_nodes * gpu_per_node

        job_info = {
            "name": name,
            "creation": creation[:19].replace("T", " ") if creation else "-",
            "nodes": total_nodes,
            "gpus": total_gpus,
        }

        if OCCUPY_NAME_PATTERN.match(name):
            occupy_jobs.append(job_info)
        else:
            other_jobs.append(job_info)

    # æ˜¾ç¤ºå å¡ä»»åŠ¡åˆ—è¡¨
    if occupy_jobs:
        table = Table(title="ğŸ”¥ å å¡ä»»åŠ¡åˆ—è¡¨", show_lines=False, border_style="cyan")
        table.add_column("#", style="dim", width=4)
        table.add_column("ä»»åŠ¡åç§°", style="bold cyan", min_width=30)
        table.add_column("èŠ‚ç‚¹æ•°", justify="center", width=8)
        table.add_column("GPU æ•°", justify="center", width=8)
        table.add_column("åˆ›å»ºæ—¶é—´", width=22)

        for i, job in enumerate(occupy_jobs, 1):
            table.add_row(
                str(i),
                job["name"],
                str(job["nodes"]),
                str(job["gpus"]),
                job["creation"],
            )

        console.print(table)
        console.print()

    if not occupy_jobs and not other_jobs:
        print_warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å å¡ä»»åŠ¡")
        return

    # æ„å»ºå¤šé€‰åˆ—è¡¨
    all_choices = []

    if occupy_jobs:
        all_choices.append({"name": f"--- å å¡ä»»åŠ¡ ({len(occupy_jobs)} ä¸ª) ---", "value": "__separator_occupy__", "enabled": False})
        # ä¸€é”®å…¨é€‰å å¡ä»»åŠ¡
        total_occupy_gpus = sum(j["gpus"] for j in occupy_jobs)
        all_choices.append({
            "name": f"âš¡ å…¨é€‰æ‰€æœ‰å å¡ä»»åŠ¡ ({len(occupy_jobs)} ä¸ª, {total_occupy_gpus} GPU)",
            "value": "__all_occupy__",
        })
        for job in sorted(occupy_jobs, key=lambda x: x["name"]):
            all_choices.append({
                "name": f"  {job['name']}  ({job['nodes']}èŠ‚ç‚¹, {job['gpus']}GPU, {job['creation']})",
                "value": job["name"],
            })

    if other_jobs:
        all_choices.append({"name": f"--- å…¶ä»–ä»»åŠ¡ ({len(other_jobs)} ä¸ª) ---", "value": "__separator_other__", "enabled": False})
        for job in sorted(other_jobs, key=lambda x: x["name"]):
            all_choices.append({
                "name": f"  {job['name']}  ({job['nodes']}èŠ‚ç‚¹, {job['gpus']}GPU, {job['creation']})",
                "value": job["name"],
            })

    all_choices.append({"name": "âŒ å–æ¶ˆ", "value": "__cancel__"})

    selected = inquirer.checkbox(
        message="è¯·é€‰æ‹©è¦åˆ é™¤çš„ä»»åŠ¡ (ç©ºæ ¼é€‰ä¸­, å›è½¦ç¡®è®¤)",
        choices=all_choices,
        pointer="â¯",
    ).execute()

    if not selected or "__cancel__" in selected:
        print_warning("å·²å–æ¶ˆ")
        return

    # å¤„ç†ã€Œå…¨é€‰å å¡ä»»åŠ¡ã€
    if "__all_occupy__" in selected:
        selected = [j["name"] for j in occupy_jobs]
    else:
        # è¿‡æ»¤æ‰åˆ†éš”ç¬¦
        selected = [s for s in selected if not s.startswith("__")]

    if not selected:
        print_warning("æœªé€‰æ‹©ä»»ä½•ä»»åŠ¡")
        return

    # æ˜¾ç¤ºå¾…åˆ é™¤åˆ—è¡¨
    total_del_gpus = 0
    console.print()
    console.print("[bold yellow]âš ï¸  å³å°†åˆ é™¤ä»¥ä¸‹ä»»åŠ¡:[/bold yellow]")
    for name in selected:
        # æŸ¥æ‰¾ GPU æ•°
        for j in occupy_jobs + other_jobs:
            if j["name"] == name:
                total_del_gpus += j["gpus"]
                console.print(f"  [bold red]âœ–[/bold red] {name}  ({j['nodes']}èŠ‚ç‚¹, {j['gpus']}GPU)")
                break
        else:
            console.print(f"  [bold red]âœ–[/bold red] {name}")

    console.print()
    console.print(f"[bold]å…± {len(selected)} ä¸ªä»»åŠ¡, {total_del_gpus} å¼  GPU å°†è¢«é‡Šæ”¾[/bold]")
    console.print()

    if not confirm_with_input("ç¡®è®¤åˆ é™¤? è¯·è¾“å…¥ 'yes'"):
        print_warning("å·²å–æ¶ˆåˆ é™¤")
        return

    # é€ä¸ªåˆ é™¤
    console.print()
    success_count = 0
    fail_count = 0
    for name in selected:
        print_info(f"æ­£åœ¨åˆ é™¤: {name}")
        rc, stdout, stderr = run_kubectl(
            ["delete", "pytorchjob", name, "--ignore-not-found=true"],
            namespace,
        )
        if rc == 0:
            print_success(f"å·²åˆ é™¤: {name}")
            success_count += 1
            # åŒæ—¶æ¸…ç†å¯¹åº”çš„ YAML æ–‡ä»¶
            yaml_path = os.path.join(OCCUPY_YAML_DIR, f"{name}.yaml")
            if os.path.exists(yaml_path):
                os.remove(yaml_path)
        else:
            print_error(f"åˆ é™¤å¤±è´¥ {name}: {stderr.strip()}")
            fail_count += 1

    console.print()
    if fail_count == 0:
        print_success(f"å…¨éƒ¨åˆ é™¤å®Œæˆ! å…±åˆ é™¤ {success_count} ä¸ªä»»åŠ¡, é‡Šæ”¾ {total_del_gpus} å¼  GPU")
    else:
        print_warning(f"åˆ é™¤å®Œæˆ: {success_count} æˆåŠŸ, {fail_count} å¤±è´¥")


def _random_task_identity() -> Tuple[str, str]:
    """éšæœºç”Ÿæˆæ¨¡å‹å’Œä»»åŠ¡ç±»å‹ç»„åˆï¼Œè¿”å› (model, task)ï¼Œå·²åš K8s å‘½ååˆè§„å¤„ç†"""
    model = random.choice(MODEL_NAMES)
    task = random.choice(TASK_TYPES)
    # K8s èµ„æºååªå…è®¸å°å†™å­—æ¯ã€æ•°å­—ã€'-'
    model = model.lower().replace(".", "").replace("_", "-")
    task = task.lower().replace("_", "-")
    return model, task


def _submit_occupy_jobs(namespace: str):
    """æäº¤æ–°çš„å å¡ä»»åŠ¡"""
    print_info("æ­£åœ¨æŸ¥è¯¢é›†ç¾¤ GPU èŠ‚ç‚¹ä¿¡æ¯...")
    console.print()

    # 1. è·å–æ‰€æœ‰ GPU èŠ‚ç‚¹
    all_nodes = _get_gpu_nodes(namespace)
    if not all_nodes:
        print_error("æœªæ‰¾åˆ°ä»»ä½• GPU èŠ‚ç‚¹")
        return

    # 2. è·å–å·²å ç”¨çš„èŠ‚ç‚¹
    busy_nodes = _get_busy_nodes(namespace)

    # 3. è®¡ç®—ç©ºé—²èŠ‚ç‚¹
    free_nodes = [n for n in all_nodes if n["name"] not in busy_nodes]

    # 4. æ˜¾ç¤ºæ€»è§ˆ
    _print_node_overview(all_nodes, busy_nodes, free_nodes)
    console.print()

    if not free_nodes:
        print_warning("æ²¡æœ‰ç©ºé—²çš„ GPU èŠ‚ç‚¹å¯ä¾›å ç”¨")
        return

    # 5. æŒ‰å®ä¾‹ç±»å‹åˆ†ç»„ç©ºé—²èŠ‚ç‚¹
    free_by_type: Dict[str, List[Dict]] = {}
    for n in free_nodes:
        itype = n["instance_type"]
        free_by_type.setdefault(itype, []).append(n)

    total_free_gpus = sum(n["gpu_count"] for n in free_nodes)
    console.print(f"[bold green]ç©ºé—²èŠ‚ç‚¹: {len(free_nodes)} ä¸ª ({total_free_gpus} å¼  GPU)[/bold green]")
    if len(free_by_type) > 1:
        for itype, nodes in sorted(free_by_type.items()):
            profile = _get_instance_profile(itype)
            console.print(f"  [cyan]{itype}[/cyan]: {len(nodes)} ä¸ª ({profile['description']})")
    console.print()

    # 6. æŒ‰å®ä¾‹ç±»å‹åˆ†æ‰¹ï¼šåŒä¸€æ‰¹æ¬¡å†…åªåŒ…å«ç›¸åŒç±»å‹çš„èŠ‚ç‚¹
    batch_plan = []       # æ¯ä¸ªå…ƒç´ : (batch_size, instance_type)
    for itype in sorted(free_by_type.keys()):
        type_free = len(free_by_type[itype])
        remaining = type_free
        while remaining > 0:
            batch = min(DEFAULT_BATCH_SIZE, remaining)
            batch_plan.append((batch, itype))
            remaining -= batch

    # 7. ä¸ºæ¯ä¸ªæ‰¹æ¬¡éšæœºç”Ÿæˆä»»åŠ¡åï¼Œé¿å…é‡å
    date_str = datetime.now().strftime("%m%d")
    existing_names = _get_existing_job_names(namespace)
    job_names = _generate_random_job_names(len(batch_plan), date_str, existing_names)

    # 8. å±•ç¤ºå å¡è®¡åˆ’
    _print_occupy_plan(batch_plan, job_names)
    console.print()

    # 9. è®©ç”¨æˆ·é€‰æ‹©è¦æäº¤çš„æ‰¹æ¬¡
    total_nodes = sum(b[0] for b in batch_plan)
    choices = []
    choices.append({"name": f"å…¨éƒ¨æäº¤ ({total_nodes} èŠ‚ç‚¹, {len(batch_plan)} ä¸ªä»»åŠ¡)", "value": "all"})
    for i, (batch_size, itype) in enumerate(batch_plan):
        profile = _get_instance_profile(itype)
        gpus = batch_size * profile["gpus"]
        choices.append({
            "name": f"ä»…ç¬¬ {i+1} æ‰¹: {job_names[i]} ({batch_size} èŠ‚ç‚¹, {gpus} GPU, {profile['description']})",
            "value": str(i),
        })
    choices.append({"name": "å–æ¶ˆ", "value": "cancel"})

    selected = inquirer.select(
        message="ä¸»äººï¼Œè¯·é€‰æ‹©å å¡æ–¹æ¡ˆ",
        choices=choices,
        pointer="â¯",
    ).execute()

    if selected == "cancel":
        print_warning("å·²å–æ¶ˆå å¡æ“ä½œ")
        return

    if selected != "all":
        batch_idx = int(selected)
        batch_plan = [batch_plan[batch_idx]]
        job_names = [job_names[batch_idx]]

    # 10. ç”Ÿæˆ YAML å¹¶ç¡®è®¤
    yaml_files = _generate_occupy_yamls(batch_plan, namespace, job_names)

    console.print()
    console.print(Panel(
        "\n".join([f"  {os.path.basename(f)}" for f in yaml_files]),
        title="ğŸ“„ å°†ç”Ÿæˆä»¥ä¸‹å å¡ä»»åŠ¡ YAML",
        border_style="cyan",
        padding=(0, 2),
    ))
    console.print()

    if not confirm("ç¡®è®¤æäº¤å å¡ä»»åŠ¡?"):
        for f in yaml_files:
            if os.path.exists(f):
                os.remove(f)
        print_warning("å·²å–æ¶ˆï¼ŒYAML æ–‡ä»¶å·²æ¸…ç†")
        return

    # 11. é€ä¸ªæäº¤
    for yaml_file in yaml_files:
        job_name = os.path.basename(yaml_file).replace(".yaml", "")
        print_info(f"æ­£åœ¨æäº¤: {job_name}")
        success, message = apply_yaml(yaml_file, namespace)
        if success:
            print_success(f"å·²æäº¤: {message}")
        else:
            print_error(f"æäº¤å¤±è´¥: {message}")

    console.print()
    total_gpus = sum(b[0] * _get_instance_profile(b[1])["gpus"] for b in batch_plan)
    total_submit_nodes = sum(b[0] for b in batch_plan)
    print_success(f"å å¡ä»»åŠ¡å…¨éƒ¨æäº¤å®Œæˆ! å…± {total_submit_nodes} èŠ‚ç‚¹, {total_gpus} å¼  GPU")
    console.print("[dim]æç¤º: ä½¿ç”¨ 'é›†ç¾¤æ¦‚å†µæ€»è§ˆ' æˆ– 'ç›‘æ§ Pods çŠ¶æ€' æŸ¥çœ‹å å¡ä»»åŠ¡å¯åŠ¨æƒ…å†µ[/dim]")


def _auto_occupy(namespace: str) -> int:
    """éäº¤äº’å¼è‡ªåŠ¨å å¡ï¼šæ£€æµ‹ç©ºé—²èŠ‚ç‚¹å¹¶å…¨éƒ¨å æ»¡ã€‚è¿”å›æœ¬æ¬¡å ç”¨çš„èŠ‚ç‚¹æ•°ï¼ˆ0 è¡¨ç¤ºæ— ç©ºé—²ï¼‰ã€‚"""
    all_nodes = _get_gpu_nodes(namespace)
    if not all_nodes:
        return 0

    busy_nodes = _get_busy_nodes(namespace)
    free_nodes = [n for n in all_nodes if n["name"] not in busy_nodes]

    if not free_nodes:
        return 0

    # æŒ‰å®ä¾‹ç±»å‹åˆ†ç»„
    free_by_type: Dict[str, List[Dict]] = {}
    for n in free_nodes:
        free_by_type.setdefault(n["instance_type"], []).append(n)

    # æŒ‰å®ä¾‹ç±»å‹åˆ†æ‰¹
    batch_plan = []  # (batch_size, instance_type)
    for itype in sorted(free_by_type.keys()):
        remaining = len(free_by_type[itype])
        while remaining > 0:
            batch = min(DEFAULT_BATCH_SIZE, remaining)
            batch_plan.append((batch, itype))
            remaining -= batch

    # ç”Ÿæˆéšæœºä»»åŠ¡å
    date_str = datetime.now().strftime("%m%d")
    existing_names = _get_existing_job_names(namespace)
    job_names = _generate_random_job_names(len(batch_plan), date_str, existing_names)

    # ç”Ÿæˆ YAML
    yaml_files = _generate_occupy_yamls(batch_plan, namespace, job_names)

    # é€ä¸ªæäº¤
    success_count = 0
    for yaml_file in yaml_files:
        job_name = os.path.basename(yaml_file).replace(".yaml", "")
        success, message = apply_yaml(yaml_file, namespace)
        if success:
            print_success(f"  âœ… å·²æäº¤: {job_name}")
            success_count += 1
        else:
            print_error(f"  âŒ æäº¤å¤±è´¥ {job_name}: {message}")

    total_nodes = sum(b[0] for b in batch_plan)
    return total_nodes if success_count > 0 else 0


def _auto_patrol(namespace: str):
    """è‡ªåŠ¨å·¡é€»æ¨¡å¼ï¼šå®šæ—¶æ£€æµ‹ç©ºé—² GPU èŠ‚ç‚¹ï¼Œå‘ç°ç©ºé—²åˆ™è‡ªåŠ¨å å¡"""
    # è®©ç”¨æˆ·é€‰æ‹©å·¡é€»é—´éš”
    interval_choice = inquirer.select(
        message="é€‰æ‹©å·¡é€»é—´éš”",
        choices=[
            {"name": "1 åˆ†é’Ÿ", "value": 60},
            {"name": "3 åˆ†é’Ÿ", "value": 180},
            {"name": "5 åˆ†é’Ÿ (æ¨è)", "value": 300},
            {"name": "10 åˆ†é’Ÿ", "value": 600},
            {"name": "è‡ªå®šä¹‰...", "value": "custom"},
        ],
        default=300,
        pointer="â¯",
    ).execute()

    if interval_choice == "custom":
        interval = int(inquirer.number(
            message="è¾“å…¥å·¡é€»é—´éš”ï¼ˆç§’ï¼‰",
            min_allowed=30,
            max_allowed=3600,
            default=300,
        ).execute())
    else:
        interval = interval_choice

    console.print()
    console.print(Panel(
        f"[bold cyan]è‡ªåŠ¨å·¡é€»æ¨¡å¼å·²å¯åŠ¨[/bold cyan]\n\n"
        f"  å·¡é€»é—´éš”: [bold]{interval}[/bold] ç§’ ({interval // 60} åˆ† {interval % 60} ç§’)\n"
        f"  æ£€æµ‹åˆ°ç©ºé—²èŠ‚ç‚¹æ—¶å°†è‡ªåŠ¨æäº¤å å¡ä»»åŠ¡\n"
        f"  æŒ‰ [bold yellow]Ctrl+C[/bold yellow] åœæ­¢å·¡é€»",
        title="ğŸ‘ï¸ è‡ªåŠ¨å·¡é€»",
        border_style="cyan",
        padding=(1, 2),
    ))
    console.print()

    round_num = 0
    total_occupied = 0

    try:
        while True:
            round_num += 1
            now = datetime.now().strftime("%H:%M:%S")
            console.print(f"[dim]â”€â”€ ç¬¬ {round_num} è½®å·¡é€» ({now}) â”€â”€[/dim]")

            try:
                all_nodes = _get_gpu_nodes(namespace)
                busy_nodes = _get_busy_nodes(namespace)
                free_count = len([n for n in all_nodes if n["name"] not in busy_nodes]) if all_nodes else 0
                total = len(all_nodes) if all_nodes else 0

                if free_count > 0:
                    console.print(f"  ğŸ” å‘ç° [bold green]{free_count}[/bold green] ä¸ªç©ºé—²èŠ‚ç‚¹ (å…± {total} ä¸ª)ï¼Œæ­£åœ¨è‡ªåŠ¨å å¡...")
                    occupied = _auto_occupy(namespace)
                    if occupied > 0:
                        total_occupied += occupied
                        print_success(f"  æœ¬è½®å ç”¨ {occupied} èŠ‚ç‚¹ï¼Œç´¯è®¡å ç”¨ {total_occupied} èŠ‚ç‚¹")
                    else:
                        print_warning("  å å¡æäº¤å¤±è´¥æˆ–èŠ‚ç‚¹å·²è¢«æŠ¢å ")
                else:
                    console.print(f"  âœ… å…¨éƒ¨ {total} èŠ‚ç‚¹å·²å æ»¡ï¼Œæ— éœ€æ“ä½œ")
            except Exception as e:
                print_error(f"  å·¡é€»å¼‚å¸¸: {e}")

            console.print(f"  [dim]ä¸‹æ¬¡å·¡é€»: {interval} ç§’å...[/dim]")
            console.print()
            time.sleep(interval)

    except KeyboardInterrupt:
        console.print()
        console.print(Panel(
            f"[bold]å·¡é€»å·²åœæ­¢[/bold]\n\n"
            f"  æ€»å·¡é€»è½®æ•°: {round_num}\n"
            f"  ç´¯è®¡å ç”¨èŠ‚ç‚¹: {total_occupied}",
            title="ğŸ‘ï¸ å·¡é€»æŠ¥å‘Š",
            border_style="yellow",
            padding=(0, 2),
        ))


def _get_gpu_nodes(namespace: str) -> List[Dict]:
    """è·å–æ‰€æœ‰å¸¦ nvidia.com/gpu èµ„æºçš„ GPU èŠ‚ç‚¹ï¼Œè‡ªåŠ¨è¯†åˆ«å®ä¾‹ç±»å‹"""
    rc, stdout, stderr = run_kubectl(
        ["get", "nodes", "-o", "json"],
        namespace,
        timeout=15,
    )
    if rc != 0:
        return []

    try:
        data = json.loads(stdout)
        nodes = []
        for item in data.get("items", []):
            metadata = item.get("metadata", {})
            labels = metadata.get("labels", {})
            status = item.get("status", {})

            # GPU å®¹é‡ â€” è·³è¿‡æ²¡æœ‰ GPU çš„èŠ‚ç‚¹
            capacity = status.get("capacity", {})
            gpu_count = int(capacity.get("nvidia.com/gpu", 0))
            if gpu_count == 0:
                continue

            # å®ä¾‹ç±»å‹ï¼šä¼˜å…ˆä½¿ç”¨æ ‡å‡† labelï¼Œå›é€€åˆ° beta label
            instance_type = (
                labels.get("node.kubernetes.io/instance-type")
                or labels.get("beta.kubernetes.io/instance-type")
                or "unknown"
            )

            conditions = status.get("conditions", [])
            ready = any(
                c.get("type") == "Ready" and c.get("status") == "True"
                for c in conditions
            )

            profile = _get_instance_profile(instance_type)

            nodes.append({
                "name": metadata.get("name", ""),
                "ready": ready,
                "gpu_count": gpu_count,
                "status": "Ready" if ready else "NotReady",
                "instance_type": instance_type,
                "gpu_model": profile["gpu_model"],
                "efa_count": profile["efa"],
                "description": profile["description"],
            })
        return nodes
    except (json.JSONDecodeError, KeyError):
        return []


def _get_busy_nodes(namespace: str) -> set:
    """è·å–å·²æœ‰ Pod å ç”¨çš„èŠ‚ç‚¹åé›†åˆ"""
    rc, stdout, stderr = run_kubectl(
        ["get", "pods", "-o", "json"],
        namespace,
        timeout=15,
    )
    if rc != 0:
        return set()

    try:
        data = json.loads(stdout)
        busy = set()
        for item in data.get("items", []):
            phase = item.get("status", {}).get("phase", "")
            # åªå…³å¿ƒ Running / Pending çš„ Podï¼ˆå®ƒä»¬å ç”¨äº†èŠ‚ç‚¹èµ„æºï¼‰
            if phase in ("Running", "Pending", "ContainerCreating"):
                node_name = item.get("spec", {}).get("nodeName", "")
                if node_name:
                    busy.add(node_name)
                # Pending çš„ Pod å¯èƒ½è¿˜æ²¡æœ‰ nodeNameï¼Œä½†ä»å ç”¨èµ„æºé…é¢
        return busy
    except (json.JSONDecodeError, KeyError):
        return set()


def _print_node_overview(all_nodes: list, busy_nodes: set, free_nodes: list):
    """æ‰“å°èŠ‚ç‚¹æ€»è§ˆè¡¨"""
    table = Table(title="ğŸ–¥ï¸  GPU èŠ‚ç‚¹æ€»è§ˆ", show_lines=False, border_style="dim")
    table.add_column("#", style="dim", width=4)
    table.add_column("èŠ‚ç‚¹åç§°", style="cyan", min_width=40)
    table.add_column("å®ä¾‹ç±»å‹", style="magenta", min_width=18)
    table.add_column("GPU", justify="center", width=10)
    table.add_column("èŠ‚ç‚¹çŠ¶æ€", justify="center", width=12)
    table.add_column("å ç”¨çŠ¶æ€", justify="center", width=12)

    free_names = set(n["name"] for n in free_nodes)

    for i, node in enumerate(sorted(all_nodes, key=lambda x: (x["instance_type"], x["name"])), 1):
        is_busy = node["name"] in busy_nodes
        occupy_status = "[red]å·²å ç”¨[/red]" if is_busy else "[green]ç©ºé—²[/green]"
        node_status = colorize_status(node["status"])
        gpu_info = f"{node['gpu_count']}Ã—{node['gpu_model']}"
        table.add_row(
            str(i),
            node["name"],
            node["instance_type"].replace("ml.", ""),
            gpu_info,
            node_status,
            occupy_status,
        )

    console.print(table)

    # æŒ‰å®ä¾‹ç±»å‹ç»Ÿè®¡
    from collections import Counter
    type_counter = Counter()
    free_type_counter = Counter()
    for n in all_nodes:
        type_counter[n["instance_type"]] += 1
    for n in free_nodes:
        free_type_counter[n["instance_type"]] += 1

    total = len(all_nodes)
    free = len(free_nodes)
    busy = total - free

    if total > 0:
        bar_width = 40
        f_len = max(1, round(free / total * bar_width)) if free else 0
        b_len = bar_width - f_len
        bar = f"[green]{'â–ˆ' * f_len}[/green][red]{'â–ˆ' * b_len}[/red]"
        console.print(f"  ç©ºé—² [green]{free}[/green] / å·²å ç”¨ [red]{busy}[/red] / æ€»è®¡ {total}")
        console.print(f"  {bar}")

        # æŒ‰ç±»å‹æ˜¾ç¤ºç©ºé—²è¯¦æƒ…
        if len(type_counter) > 1:
            console.print()
            console.print("  [bold]æŒ‰å®ä¾‹ç±»å‹:[/bold]")
            for itype in sorted(type_counter.keys()):
                t_total = type_counter[itype]
                t_free = free_type_counter.get(itype, 0)
                profile = _get_instance_profile(itype)
                status_color = "green" if t_free > 0 else "dim"
                console.print(
                    f"    [{status_color}]{itype}[/{status_color}]: "
                    f"ç©ºé—² {t_free}/{t_total} ({profile['description']})"
                )


def _get_existing_job_names(namespace: str) -> set:
    """æŸ¥è¯¢é›†ç¾¤ä¸­å·²æœ‰çš„æ‰€æœ‰ PyTorchJob åç§°"""
    rc, stdout, stderr = run_kubectl(
        ["get", "pytorchjobs", "-o", "jsonpath={.items[*].metadata.name}"],
        namespace,
        timeout=15,
    )
    if rc != 0 or not stdout.strip():
        return set()
    return set(stdout.strip().split())


def _generate_random_job_names(count: int, date_str: str, existing_names: set) -> List[str]:
    """ä¸ºæ¯ä¸ªå å¡ä»»åŠ¡éšæœºç”Ÿæˆä¸é‡å¤çš„ä»»åŠ¡åï¼Œæ ¼å¼: run-{model}-{task}-{date}-{index}"""
    # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ (model, task) ç»„åˆ
    combos = []
    for m in MODEL_NAMES:
        for t in TASK_TYPES:
            model = m.lower().replace(".", "").replace("_", "-")
            task = t.lower().replace("_", "-")
            combos.append((model, task))
    random.shuffle(combos)

    names = []
    combo_idx = 0
    for _ in range(count):
        # æ‰¾ä¸€ä¸ªä¸å†²çªçš„åå­—
        found = False
        attempts = 0
        while attempts < len(combos) * 100:
            model, task = combos[combo_idx % len(combos)]
            combo_idx += 1
            # æ‰¾åˆ°è¯¥å‰ç¼€ä¸‹çš„æœ€å¤§ç¼–å·
            prefix = f"run-{model}-{task}-{date_str}"
            max_idx = 0
            for existing in existing_names | set(names):
                m = re.match(rf"^{re.escape(prefix)}-(\d+)$", existing)
                if m:
                    max_idx = max(max_idx, int(m.group(1)))
            job_name = f"{prefix}-{max_idx + 1:02d}"
            if job_name not in existing_names and job_name not in names:
                names.append(job_name)
                found = True
                break
            attempts += 1
        if not found:
            # fallback
            fallback_name = f"run-qwen3-retool-{date_str}-{random.randint(50, 99):02d}"
            names.append(fallback_name)

    return names


def _print_occupy_plan(batch_plan: list, job_names: list):
    """æ‰“å°å å¡è®¡åˆ’ã€‚batch_plan å…ƒç´ ä¸º (batch_size, instance_type)"""
    table = Table(title="ğŸ“‹ å å¡è®¡åˆ’", show_lines=False, border_style="cyan")
    table.add_column("æ‰¹æ¬¡", style="bold", width=6)
    table.add_column("ä»»åŠ¡å", style="cyan", min_width=35)
    table.add_column("å®ä¾‹ç±»å‹", style="magenta", min_width=18)
    table.add_column("èŠ‚ç‚¹æ•°", justify="center", width=8)
    table.add_column("Master", justify="center", width=8)
    table.add_column("Worker", justify="center", width=8)
    table.add_column("GPU æ•°", justify="center", width=8)

    total_gpus = 0
    for i, (batch_size, itype) in enumerate(batch_plan):
        worker_count = batch_size - 1
        profile = _get_instance_profile(itype)
        gpus = batch_size * profile["gpus"]
        total_gpus += gpus
        table.add_row(
            f"#{i+1}",
            job_names[i],
            itype.replace("ml.", ""),
            str(batch_size),
            "1",
            str(worker_count),
            str(gpus),
        )

    total_nodes = sum(b[0] for b in batch_plan)
    table.add_row(
        "[bold]åˆè®¡[/bold]",
        f"[bold]{len(batch_plan)} ä¸ªä»»åŠ¡[/bold]",
        "",
        f"[bold]{total_nodes}[/bold]",
        f"[bold]{len(batch_plan)}[/bold]",
        f"[bold]{total_nodes - len(batch_plan)}[/bold]",
        f"[bold]{total_gpus}[/bold]",
    )

    console.print(table)


def _generate_occupy_yamls(batch_plan: list, namespace: str, job_names: list) -> List[str]:
    """æ ¹æ®æ‰¹æ¬¡è®¡åˆ’ç”Ÿæˆå å¡ YAML æ–‡ä»¶ã€‚batch_plan å…ƒç´ ä¸º (batch_size, instance_type)"""
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OCCUPY_YAML_DIR, exist_ok=True)

    yaml_files = []

    for i, (batch_size, itype) in enumerate(batch_plan):
        worker_count = batch_size - 1  # 1 ä¸ª Master + N ä¸ª Worker
        job_name = job_names[i]
        yaml_content = _build_occupy_yaml(job_name, namespace, worker_count, itype)
        
        output_path = os.path.join(OCCUPY_YAML_DIR, f"{job_name}.yaml")
        with open(output_path, "w") as f:
            f.write(yaml_content)
        yaml_files.append(output_path)

    return yaml_files


def _build_occupy_yaml(job_name: str, namespace: str, worker_replicas: int, instance_type: str) -> str:
    """æ„å»ºå å¡ PyTorchJob YAML å†…å®¹ï¼Œæ ¹æ®å®ä¾‹ç±»å‹åŠ¨æ€è°ƒæ•´èµ„æºé…ç½®"""
    profile = _get_instance_profile(instance_type)
    gpu_count = profile["gpus"]
    efa_count = profile["efa"]

    # å…¬å…±çš„å¯åŠ¨å‘½ä»¤
    occupy_cmd = """echo "=== GPU Occupy - {role} ==="
                  echo "RANK: $RANK"
                  echo "WORLD_SIZE: $WORLD_SIZE"
                  echo "Instance Type: """ + instance_type + """"
                  hostname -I
                  nvidia-smi

                  # æ¸…ç† PyTorchJob æ³¨å…¥çš„åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡
                  unset MASTER_ADDR MASTER_PORT WORLD_SIZE RANK LOCAL_RANK
                  unset GROUP_RANK ROLE_RANK LOCAL_WORLD_SIZE ROLE_WORLD_SIZE

                  source /root/miniconda3/etc/profile.d/conda.sh
                  conda activate agent-lightning
                  export PATH=$CONDA_PREFIX/bin:$PATH

                  echo "Python: $(which python3)"
                  echo "CUDA available: $(python3 -c 'import torch; print(torch.cuda.is_available())')"
                  echo "GPU count: $(python3 -c 'import torch; print(torch.cuda.device_count())')"

                  python3 /fsx/gpu_stress.py \\
                    --matrix-size 4096 \\
                    --duty-cycle 0.80 \\
                    --mem-fraction 0.75 \\
                    --duration 168

                  EXIT_CODE=$?
                  echo "gpu_stress.py exited with code: $EXIT_CODE"
                  if [ $EXIT_CODE -ne 0 ]; then
                    echo "[ERROR] gpu_stress.py failed!"
                  fi
                  sleep 365d"""

    master_cmd = occupy_cmd.format(role="Master")
    worker_cmd = occupy_cmd.format(role="Worker")

    # å…¬å…±çš„ volumeMounts
    volume_mounts = [
        {"name": "shmem", "mountPath": "/dev/shm"},
        {"name": "local", "mountPath": "/local"},
        {"name": "inst-nvme", "mountPath": "/ckpt-path"},
        {"name": "local-cache", "mountPath": "/root/.cache"},
        {"name": "fsx-storage", "mountPath": "/fsx", "subPath": "youtu-agent/zhijianzhou"},
    ]

    # å…¬å…±çš„ volumes
    volumes = [
        {"name": "shmem", "hostPath": {"path": "/dev/shm"}},
        {"name": "local", "hostPath": {"path": "/mnt/k8s-disks/0"}},
        {"name": "local-cache", "hostPath": {"path": "/opt/dlami/nvme/.cache"}},
        {"name": "inst-nvme", "hostPath": {"path": "/opt/dlami/nvme/checkpoints/"}},
        {"name": "fsx-storage", "persistentVolumeClaim": {"claimName": "fsx-claim"}},
    ]

    # å…¬å…±ç¯å¢ƒå˜é‡
    env = [
        {"name": "FI_PROVIDER", "value": "efa"},
        {"name": "FI_EFA_USE_DEVICE_RDMA", "value": "1"},
        {"name": "PATH", "value": "/root/miniconda3/envs/agent-lightning/bin:/root/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"},
    ]

    # æ ¹æ®å®ä¾‹ç±»å‹åŠ¨æ€é…ç½®èµ„æºè¯·æ±‚
    resources = {
        "requests": {"nvidia.com/gpu": gpu_count, "vpc.amazonaws.com/efa": efa_count},
        "limits": {"nvidia.com/gpu": gpu_count, "vpc.amazonaws.com/efa": efa_count},
    }

    image = "054486717055.dkr.ecr.ap-southeast-3.amazonaws.com/youtu-agent:agent-lightning-0.2.2-1218-aws"

    # æ„å»º Master spec
    master_container = {
        "name": "pytorch",
        "image": image,
        "imagePullPolicy": "Always",
        "ports": [
            {"containerPort": 6379, "name": "gcs-server"},
            {"containerPort": 8265, "name": "dashboard"},
            {"containerPort": 10001, "name": "client"},
            {"containerPort": 8000, "name": "serve"},
            {"containerPort": 8080, "name": "metrics"},
        ],
        "resources": resources,
        "env": env,
        "command": ["bash", "-c"],
        "args": [master_cmd],
        "volumeMounts": volume_mounts,
    }

    # æ„å»º Worker spec
    worker_container = {
        "name": "pytorch",
        "image": image,
        "imagePullPolicy": "Always",
        "resources": resources,
        "env": env,
        "command": ["bash", "-c"],
        "args": [worker_cmd],
        "volumeMounts": volume_mounts,
    }

    # æ ¹æ®å®ä¾‹ç±»å‹è®¾ç½® nodeSelector
    node_selector = {"node.kubernetes.io/instance-type": instance_type}

    pytorchjob = {
        "apiVersion": "kubeflow.org/v1",
        "kind": "PyTorchJob",
        "metadata": {
            "name": job_name,
            "namespace": namespace,
        },
        "spec": {
            "nprocPerNode": str(gpu_count),
            "pytorchReplicaSpecs": {
                "Master": {
                    "replicas": 1,
                    "restartPolicy": "OnFailure",
                    "template": {
                        "spec": {
                            "nodeSelector": node_selector,
                            "containers": [master_container],
                            "volumes": volumes,
                        }
                    },
                },
            },
        },
    }

    # åªåœ¨æœ‰ Worker æ—¶æ·»åŠ  Worker spec
    if worker_replicas > 0:
        pytorchjob["spec"]["pytorchReplicaSpecs"]["Worker"] = {
            "replicas": worker_replicas,
            "restartPolicy": "OnFailure",
            "template": {
                "spec": {
                    "nodeSelector": node_selector,
                    "containers": [worker_container],
                    "volumes": volumes,
                }
            },
        }

    return yaml.dump(pytorchjob, default_flow_style=False, sort_keys=False, allow_unicode=True)
